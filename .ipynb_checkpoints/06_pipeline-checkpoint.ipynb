{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='06-nb'></a>\n",
    "\n",
    "# Music Recommender\n",
    "## Part 6: SageMaker Pipelines\n",
    "----\n",
    "In this final notebook, we'll combine all the steps we've gone over in each individual notebook, and condense them down into a [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html) object which will automate  the entire modeling process from the beginning of data ingestion to monitoring the model. SageMaker Pipelines is a tool for building machine learning pipelines that take advantage of direct SageMaker integration. Because of this integration, you can create a pipeline and set up SageMaker Projects for orchestration using a tool that handles much of the step creation and management for you.\n",
    "\n",
    "----\n",
    "### Contents\n",
    "- [Overview](00_overview_arch_data.ipynb)\n",
    "- [Part 1: Data Prep using Data Wrangler](01_music_dataprep.flow)\n",
    "- [Part 2a: Feature Store Creation - Tracks](02a_export_fg_tracks.ipynb)\n",
    "- [Part 2b: Feature Store Creation - User Preferences](02b_export_fg_5star_features.ipynb)\n",
    "- [Part 2c: Feature Store Creation - Ratings](02c_fg_create_ratings.ipynb)\n",
    "- [Part 3: Train Model with Debugger Hooks. Set Artifacts and Register Model.](03_train_model_lineage_registry_debugger.ipynb)\n",
    "- [Part 4: Deploy Model & Inference using Online Feature Store](04_deploy_inference_explainability.ipynb)\n",
    "- [Part 5: Model Monitor](05_model_monitor.ipynb)\n",
    "- [Part 6: SageMaker Pipelines](06_pipeline.ipynb)\n",
    "    - [Architecture](#06-arch)\n",
    "    - [Pipelines Overview](#pipelines)\n",
    "    - [Cleanup](#06-cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything you may need to create them again or it may be your first time running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "dw_ecrlist                           -> {'region': {'us-west-2': '174368400705', 'us-east-\n",
      "fg_name_ratings                      -> 'ratings-feature-group-13-03-22-53-fcdc820e'\n",
      "fg_name_tracks                       -> 'track-features-13-03-22-53-fcdc820e'\n",
      "fg_name_user_preferences             -> 'user-5star-track-features-13-03-22-53-fcdc820e'\n",
      "flow_export_id                       -> '13-03-22-53-fcdc820e'\n",
      "flow_s3_uri                          -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
      "model_path                           -> 's://sagemaker-us-west-1-738335684114/music-recomm\n",
      "prefix                               -> 'music-recommendation'\n",
      "ratings_data_source                  -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
      "s3_output_path                       -> 's3://sagemaker-us-west-1-738335684114'\n",
      "tracks_data_source                   -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
      "train_data_uri                       -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
      "training_job_name                    -> 'sagemaker-xgboost-2021-06-13-23-56-09-766'\n",
      "val_data_uri                         -> 's3://sagemaker-us-west-1-738335684114/music-recom\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stored variables and their in-db values:\n",
    "dw_ecrlist                           -> {'region': {'us-west-2': '174368400705', 'us-east-\n",
    "fg_name_ratings                      -> 'ratings-feature-group-13-03-22-53-fcdc820e'\n",
    "fg_name_tracks                       -> 'track-features-13-03-22-53-fcdc820e'\n",
    "fg_name_user_preferences             -> 'user-5star-track-features-13-03-22-53-fcdc820e'\n",
    "flow_export_id                       -> '13-03-22-53-fcdc820e'\n",
    "flow_s3_uri                          -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
    "model_path                           -> 's://sagemaker-us-west-1-738335684114/music-recomm\n",
    "prefix                               -> 'music-recommendation'\n",
    "ratings_data_source                  -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
    "s3_output_path                       -> 's3://sagemaker-us-west-1-738335684114'\n",
    "tracks_data_source                   -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
    "train_data_uri                       -> 's3://sagemaker-us-west-1-738335684114/music-recom\n",
    "training_job_name                    -> 'sagemaker-xgboost-2021-06-13-23-56-09-766'\n",
    "val_data_uri                         -> 's3://sagemaker-us-west-1-738335684114/music-recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -Uq pip\n",
    "# !python -m pip install -q awswrangler==2.2.0 imbalanced-learn==0.7.0 sagemaker==2.23.1 boto3==1.16.48\n",
    "!python -m pip install -q sagemaker==2.45.0 imbalanced-learn awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "import demo_helpers  # our custom set of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.45.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region and boto3 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "sys.path.insert(1, './code')\n",
    "from parameter_store import ParameterStore\n",
    "ps = ParameterStore()\n",
    "\n",
    "parameter = ps.read('music-rec')\n",
    "pprint.pprint(parameter)\n",
    "\n",
    "dw_ecrlist = parameter['dw_ecrlist']\n",
    "fg_name_ratings = parameter['fg_name_ratings']\n",
    "fg_name_tracks = parameter['fg_name_tracks']\n",
    "fg_name_user_preferences = parameter['fg_name_user_preferences']\n",
    "\n",
    "flow_export_id = parameter['flow_export_id']\n",
    "flow_s3_uri = parameter['flow_s3_uri']\n",
    "model_path = parameter['model_path']\n",
    "prefix = parameter['prefix']\n",
    "ratings_data_source = parameter['ratings_data_source']\n",
    "tracks_data_source = parameter['tracks_data_source']\n",
    "\n",
    "\"\"\"\n",
    "endpoint_name = parameter['endpoint_name']\n",
    "feature_names = parameter['feature_names']\n",
    "fs_name_ratings = parameter['fs_name_ratings']\n",
    "fs_name_tracks = parameter['fs_name_tracks']\n",
    "fs_name_user_preferences = parameter['fs_name_user_preferences']\n",
    "model_name = parameter['model_name']\n",
    "model_packages = parameter['model_packages']\n",
    "\n",
    "mpg_name = parameter['mpg_name']\n",
    "num_training_samples = parameter['num_training_samples']\n",
    "pipeline_name = parameter['pipeline_name']\n",
    "\n",
    "\n",
    "s3_output_path = parameter['s3_output_path']\n",
    "\n",
    "train_data_uri = parameter['train_data_uri']\n",
    "training_job_name = parameter['training_job_name']\n",
    "tuning_job_name = parameter['tuning_job_name']\n",
    "val_data_uri = parameter['val_data_uri']\n",
    "best_training_job_name = parameter['best_training_job_name']\n",
    "deploy_instance_type = parameter['deploy_instance_type']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======> Output_paths\n",
    "explainability_output_path = f's3://{bucket}/{prefix}/clarify-explainability'\n",
    "\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "create_dataset_script_uri = f's3://{bucket}/{prefix}/code/create_datasets.py'\n",
    "create_feature_store_script_uri = f's3://{bucket}/{prefix}/code/feature_store_ingest.py'\n",
    "deploy_model_script_uri = f's3://{bucket}/{prefix}/code/deploy_model.py'\n",
    "model_monitor_script_uri = f's3://{bucket}/{prefix}/code/model_monitor.py'\n",
    "\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file. \n",
    "# You can change to a different node ID to export a different step in the flow file\n",
    "output_name_tracks = \"d0d4f05a-3031-4438-867b-c5fd033d6c15.default\"\n",
    "output_name_user_preferences = \"3098f603-08d2-4319-83f5-55f509eeab60.default\"\n",
    "output_name_ratings = \"8b7e0be2-7b25-4c06-82eb-c0547efebf67.default\"\n",
    "\n",
    "#======> variables used for parameterizing the notebook run\n",
    "flow_instance_count = 2\n",
    "flow_instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='06-arch'> </a>\n",
    "\n",
    "## Architecture: Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment\n",
    "##### [back to top](#06-nb)\n",
    "----\n",
    "\n",
    "![arch diagram](./images/music-rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipelines'></a>\n",
    "\n",
    "## SageMaker Pipeline Overview\n",
    "#### [back to top](#06-nb)\n",
    "---- \n",
    "\n",
    "#### [Step 1: Data Wrangler Preprocessing Step](#data-wrangler)\n",
    "#### [Step 2: Dataset and train test split](#dataset-train-test)\n",
    "#### [Step 3: Train XGboost Model](#pipe-train-xgb)\n",
    "#### [Step 4: Model Pre-deployment](#pipe-pre-deploy)\n",
    "#### [Step 5: Register Model](#pipe-Register-Model)\n",
    "#### [Step 6: Deploy Model](#deploy)\n",
    "#### [Combine Steps and Run Pipeline](#combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you've manually done each step in our machine learning workflow, you can certain steps to allow for faster model experimentation without sacrificing transparncy and model tracking. In this section you will create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but be able to change the parameters to those steps at execution without having to re-define the pipeline. This can be achieved by using ParameterInteger, ParameterFloat or ParameterString to define a value upfront which can be modified when you call `pipeline.start(parameters=parameters)` later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-wrangler'></a>\n",
    "### Step 1: Data Wranger Preprocessing Step\n",
    "[Pipeline Overview](#pipelines)\n",
    "\n",
    "#### Upload flow to S3\n",
    "This will become an input to the first step and, as such, needs to be in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler flow 01_music_dataprep.flow uploaded to s3://sagemaker-us-west-1-738335684114/music-recommendation/dataprep-notebooks/music_dataprep.flow\n"
     ]
    }
   ],
   "source": [
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"01_music_dataprep.flow\"\n",
    "\n",
    "s3_client.upload_file(Filename=flow_file_name, Bucket=bucket, Key=f'{prefix}/dataprep-notebooks/music_dataprep.flow')\n",
    "flow_s3_uri = f's3://{bucket}/{prefix}/dataprep-notebooks/music_dataprep.flow'\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the first Data Wrangler step's inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = []\n",
    "\n",
    "## Input - S3 Source: tracks.csv\n",
    "data_sources.append(ProcessingInput(\n",
    "    source=f\"s3://{bucket}/{prefix}/tracks.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=f\"{processing_dir}/tracks.csv\",\n",
    "    input_name=\"tracks.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))\n",
    "\n",
    "## Input - S3 Source: ratings.csv\n",
    "data_sources.append(ProcessingInput(\n",
    "    source=f\"s3://{bucket}/{prefix}/ratings.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=f\"{processing_dir}/ratings.csv\",\n",
    "    input_name=\"ratings.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))\n",
    "\n",
    "## Input - Flow: 01_music_dataprep.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=f\"{processing_dir}/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define outputs for the Data Wranger step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_output_tracks = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_tracks,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_tracks)\n",
    "    )\n",
    "\n",
    "flow_output_user_preferences = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_user_preferences,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_user_preferences)\n",
    "    )\n",
    "\n",
    "flow_output_ratings = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_ratings,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_ratings)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output configuration used as processing job container arguments \n",
    "output_config_tracks = {\n",
    "    output_name_tracks: {\n",
    "        \"content_type\": \"CSV\"\n",
    "    }\n",
    "}\n",
    "\n",
    "output_config_user_preferences = {\n",
    "    output_name_user_preferences: {\n",
    "        \"content_type\": \"CSV\"\n",
    "    }\n",
    "}\n",
    "\n",
    "output_config_ratings = {\n",
    "    output_name_ratings: {\n",
    "        \"content_type\": \"CSV\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dw_ecrlist' (dict)\n"
     ]
    }
   ],
   "source": [
    "dw_ecrlist = {\n",
    "    'region':{'us-west-2':'174368400705',\n",
    "              'us-east-2':'415577184552',\n",
    "              'us-west-1': '926135532090'\n",
    "             }\n",
    "}\n",
    "\n",
    "ps.store({'dw_ecrlist':dw_ecrlist},namespace='music-rec')\n",
    "ps.store()\n",
    "\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = f\"{dw_ecrlist['region'][region]}.dkr.ecr.{region}.amazonaws.com/sagemaker-data-wrangler-container:1.x\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define processor and processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangler Container URL\n",
    "# You can find the proper container uri by exporting your Data Wrangler flow to a pipeline notebook\n",
    "# container_uri = \"415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "\n",
    "\n",
    "flow_processor = sagemaker.processing.Processor(\n",
    "    role=sagemaker_role, \n",
    "    image_uri=container_uri, \n",
    "    instance_count=flow_instance_count, \n",
    "    instance_type=flow_instance_type, \n",
    "    volume_size_in_gb=30,\n",
    "    network_config=NetworkConfig(enable_network_isolation=False),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "flow_step_tracks = ProcessingStep(\n",
    "    name='DataWranglerStepTracks', \n",
    "    processor=flow_processor, \n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[flow_output_tracks],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_tracks)}'\"],\n",
    ")\n",
    "\n",
    "flow_step_user_preferences = ProcessingStep(\n",
    "    name='DataWranglerStepUserPref', \n",
    "    processor=flow_processor, \n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[flow_output_user_preferences],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_user_preferences)}'\"]\n",
    ")\n",
    "\n",
    "flow_step_ratings = ProcessingStep(\n",
    "    name='DataWranglerStepRatings', \n",
    "    processor=flow_processor, \n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[flow_output_ratings],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_ratings)}'\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset-train-test'></a>\n",
    "### Step 2: Create Dataset and Train/Test Split\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='create_datasets.py', Bucket=bucket, Key=f'{prefix}/code/create_datasets.py')\n",
    "\n",
    "create_dataset_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name='music-recommendation-split-data',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name='SplitData',\n",
    "    processor=create_dataset_processor,\n",
    "    outputs = [\n",
    "        sagemaker.processing.ProcessingOutput(output_name='train_data', source=f'{processing_dir}/output/train'),\n",
    "        sagemaker.processing.ProcessingOutput(output_name='test_data',  source=f'{processing_dir}/output/test')\n",
    "    ],\n",
    "    job_arguments=[\"--feature-group-name-tracks\", fg_name_tracks,\n",
    "                   \"--feature-group-name-ratings\", fg_name_ratings,\n",
    "                   \"--feature-group-name-user-preferences\", fg_name_user_preferences,\n",
    "                   \"--bucket-name\", bucket,\n",
    "                   \"--bucket-prefix\", prefix,\n",
    "                   \"--region\", region\n",
    "                  ],\n",
    "    code=create_dataset_script_uri\n",
    ")\n",
    "\n",
    "create_dataset_step.add_depends_on([flow_step_tracks.name, flow_step_user_preferences.name,flow_step_ratings.name ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-train-xgb'></a>\n",
    "### Step 3: Train XGBoost Model\n",
    "In this step we use the ParameterString `train_instance_param` defined at the beginning of the pipeline.\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.debugger import Rule, rule_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"4\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"100\"\n",
    "}\n",
    "\n",
    "save_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator = Estimator(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=2,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region, \"0.90-2\"),\n",
    "    hyperparameters=hyperparameters,\n",
    "    max_run=1800,\n",
    "\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=s3_output_path,  \n",
    "        collection_configs=[\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"feature_importance\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"full_shap\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"average_shap\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": str(save_interval * 2),\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = TrainingStep(\n",
    "    name='TrainStep',\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs['test_data'].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-pre-deploy'></a>\n",
    "### Step 4: Model Pre-Deployment Step\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name='music-recommender-xgboost-model',\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-Register-Model'></a>\n",
    "### Step 5: Register Model\n",
    "In this step you will use the ParameterString `model_approval_status` defined at the outset of the pipeline code.\n",
    "\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpg_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a902b6ececf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minference_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ml.t2.medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ml.m5.xlarge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtransform_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ml.m5.xlarge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_package_group_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmpg_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mapproval_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_approval_status\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mpg_name' is not defined"
     ]
    }
   ],
   "source": [
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "### Step 6: Deploy Model\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='deploy_model.py', Bucket=bucket, Key=f'{prefix}/code/deploy_model.py')\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name='music-recommender-deploy-model',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name='DeployModel',\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\", create_model_step.properties.ModelName, \n",
    "        \"--region\", region,\n",
    "        \"--endpoint-instance-type\", deploy_model_instance_type,\n",
    "        \"--endpoint-name\", \"music-recommender-model-pipeline\"],\n",
    "    code=deploy_model_script_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='monitor'></a>\n",
    "### Step 7: Model Monitor\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='model_monitor.py', Bucket=bucket, Key=f'{prefix}/code/model_monitor.py')\n",
    "\n",
    "\n",
    "model_monitor_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    base_job_name='music-recommendation-model-monitor',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "model_monitor_step = ProcessingStep(\n",
    "    name='ModelMonitor',\n",
    "    processor=model_monitor_processor,\n",
    "    outputs = [\n",
    "        sagemaker.processing.ProcessingOutput(output_name='model_baseline', source=f'{processing_dir}/output/baselineresults')\n",
    "    ],\n",
    "    job_arguments=[\"--baseline-data-uri\", val_data_uri,\n",
    "                   \"--bucket-name\", bucket,\n",
    "                   \"--bucket-prefix\", prefix,\n",
    "                   \"--region\", region,\n",
    "                   \"--endpoint\", \"music-recommender-model-pipeline\"\n",
    "                  ],\n",
    "    code=model_monitor_script_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combine'></a>\n",
    "\n",
    "### Combine the Pipeline Steps and Run\n",
    "[Pipeline Overview](#pipelines)\n",
    "\n",
    "Once all of our steps are defined, we can put them together using the SageMaker `Pipeline` object. While we pass the steps in order so that it is easier to read, technically the order that we pass them does not matter since the pipeline DAG will parse it out properly based on any dependencies between steps. If the input of one step is the output of another step, the Pipelines understands which must come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f'MusicRecommendationPipeline'\n",
    "\n",
    "ps.store({'pipeline_name':pipeline_name},namespace='music-rec')\n",
    "ps.store()\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param, \n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "        flow_step_tracks,\n",
    "        flow_step_user_preferences,\n",
    "        flow_step_ratings,\n",
    "        create_dataset_step,\n",
    "        train_step, \n",
    "        create_model_step, \n",
    "        register_step,\n",
    "        deploy_step,\n",
    "        model_monitor_step  # turning off this step until we can fix depends_on otherwise it fails \n",
    "    ])\n",
    "\n",
    "\"\"\"\n",
    "training_step.add_depends_on([processing_step_2])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline definition to the SageMaker Pipeline service\n",
    "Note: If an existing pipeline has the same name it will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the entire pipeline definition\n",
    "Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#json.loads(pipeline.describe()['PipelineDefinition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Note this will take about 70-80 minutes to complete. You can watch the progress of the Pipeline Job on your SageMaker Studio Components panel\n",
    "\n",
    "<!-- ![image.png](attachment:image.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the same feature group names --- fg_name_ratings, fg_name_tracks, fg_name_user_preferences --- we created in the `02` notebooks and using the data in our S3 bucket to create an [Offline Feature Store](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html). In offline mode, large streams of data are fed to an offline store, which can be used for training and batch inference. This mode requires a feature group to be stored in an offline store. The offline store uses your S3 bucket for storage and can also fetch data using Athena queries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_names = [fg_name_ratings, fg_name_tracks, fg_name_user_preferences]\n",
    "feature_groups = []\n",
    "for name in feature_group_names:\n",
    "    feature_group = FeatureGroup(name=name, sagemaker_session=feature_store_session)\n",
    "    feature_groups.append(feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_s3_prefixes = []\n",
    "for feature_group in feature_groups:\n",
    "    feature_group_table_name = feature_group.describe().get(\"OfflineStoreConfig\").get(\"DataCatalogConfig\").get(\"TableName\")\n",
    "    feature_group_s3_prefix = f'{account_id}/sagemaker/{region}/offline-store/{feature_group_table_name}'\n",
    "    feature_group_s3_prefixes.append(feature_group_s3_prefix)\n",
    "\n",
    "# wait for data to be added to offline feature store\n",
    "def wait_for_offline_store(feature_group_s3_prefix):\n",
    "    print(feature_group_s3_prefix)\n",
    "    offline_store_contents = None\n",
    "    while (offline_store_contents is None):\n",
    "        objects_in_bucket = s3_client.list_objects(Bucket=bucket, Prefix=feature_group_s3_prefix)\n",
    "        if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "            offline_store_contents = objects_in_bucket['Contents']\n",
    "        else:\n",
    "            print('Waiting for data in offline store...')\n",
    "            time.sleep(60)\n",
    "    print('Data available:', feature_group_s3_prefix)\n",
    "    \n",
    "for s3_prefix in feature_group_s3_prefixes:\n",
    "    wait_for_offline_store(s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special pipeline parameters can be defined or changed here\n",
    "parameters = {'TrainingInstance': 'ml.m5.xlarge'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the notebook, we defines several `ProcessingStep()`s and a `TrainingStep()` which our `Pipeline()` instance here will reference and kick off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = pipeline.start(parameters=parameters)\n",
    "start_response.wait(delay=60, max_attempts=200)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion we can use Sagemaker Studio's **Components and Registries** tab to see our Pipeline graph and any further error or log messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='06-cleanup'></a>\n",
    "\n",
    "## Cleanup\n",
    "#### [back to top](#06-nb)\n",
    "---- \n",
    "\n",
    "After running the demo, you should remove the resources which were created. You can also delete all the objects in the project's S3 directory by passing the keyword argument `delete_s3_objects=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when ran, this cell will delete all the resources created by this demo\n",
    "\n",
    "'''\n",
    "demo_helpers.delete_project_resources(\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    endpoint_name=endpoint_name, \n",
    "    pipeline_name=pipeline_name, \n",
    "    mpg_name=mpg_name, \n",
    "    prefix=prefix,\n",
    "    delete_s3_objects=True,\n",
    "    bucket_name=bucket\n",
    ")\n",
    "\n",
    "# delete feature stores within SageMaker Studio\n",
    "for fg_name in [fg_name_ratings,fg_name_tracks,fg_name_user_preferences]:\n",
    "    feature_group = FeatureGroup(name=fg_name, sagemaker_session=feature_store_session)\n",
    "    feature_group.delete()\n",
    "    print(\"Deleted feature group: {}\".format(fg_name))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for fg_name in ['ratings-feature-group-10-01-19-04-064a7180','user-5star-track-features-10-01-19-04-064a7180','track-features-10-01-19-04-064a7180',\n",
    "                'ratings-feature-group-08-21-40-56','user-5star-track-features-08-19-40-44-dca8d52b','track-features-08-19-33-28-6011d906',\n",
    "                'ratings-feature-group-08-16-40-57-eb381417','user-5star-track-features-08-16-40-57-eb381417','track-features-08-16-40-57-eb381417'\n",
    "               ]:\n",
    "    feature_group = FeatureGroup(name=fg_name, sagemaker_session=sagemaker_session)\n",
    "    feature_group.delete()\n",
    "    print(\"Deleted feature group: {}\".format(fg_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
